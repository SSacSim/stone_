{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/sdb/anaconda_env/dacon/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import cv2\n",
    "import timm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# 이미지 증량 및 처리를 위한 라이브러리 ( albumentations )\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "# sklearn \n",
    "from sklearn.model_selection import train_test_split # train test Split \n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score # f1 score \n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'IMG_SIZE':448,\n",
    "    'EPOCHS':50,\n",
    "    'LEARNING_RATE':1e-5,\n",
    "    'BATCH_SIZE':8,\n",
    "    'SEED':1042\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_list = glob.glob('../data/train/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['img_path', 'rock_type'])\n",
    "df['img_path'] = all_img_list\n",
    "df['rock_type'] = df['img_path'].apply(lambda x : str(x).split('/')[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rock_type\n",
       "Granite           92923\n",
       "Mud_Sandstone     89467\n",
       "Gneiss            73914\n",
       "Andesite          43802\n",
       "Weathered_Rock    37169\n",
       "Basalt            26810\n",
       "Etc               15935\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rock_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>rock_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>320509</th>\n",
       "      <td>../data/train/Gneiss/TRAIN_11986.jpg</td>\n",
       "      <td>Gneiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250677</th>\n",
       "      <td>../data/train/Mud_Sandstone/TRAIN_71853.jpg</td>\n",
       "      <td>Mud_Sandstone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328030</th>\n",
       "      <td>../data/train/Gneiss/TRAIN_15959.jpg</td>\n",
       "      <td>Gneiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9472</th>\n",
       "      <td>../data/train/Andesite/TRAIN_42719.jpg</td>\n",
       "      <td>Andesite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>../data/train/Andesite/TRAIN_26090.jpg</td>\n",
       "      <td>Andesite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22602</th>\n",
       "      <td>../data/train/Andesite/TRAIN_02037.jpg</td>\n",
       "      <td>Andesite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174719</th>\n",
       "      <td>../data/train/Weathered_Rock/TRAIN_02804.jpg</td>\n",
       "      <td>Weathered_Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357512</th>\n",
       "      <td>../data/train/Gneiss/TRAIN_32934.jpg</td>\n",
       "      <td>Gneiss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263260</th>\n",
       "      <td>../data/train/Mud_Sandstone/TRAIN_51603.jpg</td>\n",
       "      <td>Mud_Sandstone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324240</th>\n",
       "      <td>../data/train/Gneiss/TRAIN_36615.jpg</td>\n",
       "      <td>Gneiss</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            img_path       rock_type\n",
       "320509          ../data/train/Gneiss/TRAIN_11986.jpg          Gneiss\n",
       "250677   ../data/train/Mud_Sandstone/TRAIN_71853.jpg   Mud_Sandstone\n",
       "328030          ../data/train/Gneiss/TRAIN_15959.jpg          Gneiss\n",
       "9472          ../data/train/Andesite/TRAIN_42719.jpg        Andesite\n",
       "7622          ../data/train/Andesite/TRAIN_26090.jpg        Andesite\n",
       "...                                              ...             ...\n",
       "22602         ../data/train/Andesite/TRAIN_02037.jpg        Andesite\n",
       "174719  ../data/train/Weathered_Rock/TRAIN_02804.jpg  Weathered_Rock\n",
       "357512          ../data/train/Gneiss/TRAIN_32934.jpg          Gneiss\n",
       "263260   ../data/train/Mud_Sandstone/TRAIN_51603.jpg   Mud_Sandstone\n",
       "324240          ../data/train/Gneiss/TRAIN_36615.jpg          Gneiss\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "train, val, _, _ = train_test_split(df, df['rock_type'], test_size=0.3, stratify=df['rock_type'], random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rock_type\n",
       "Granite           0.2354\n",
       "Mud_Sandstone     0.2332\n",
       "Gneiss            0.1980\n",
       "Andesite          0.1240\n",
       "Weathered_Rock    0.0968\n",
       "Basalt            0.0694\n",
       "Etc               0.0432\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rock_type\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "train['rock_type'] = le.fit_transform(train['rock_type'])\n",
    "val['rock_type'] = le.transform(val['rock_type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        \n",
    "        image = cv2.imread(img_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        if self.label_list is not None:\n",
    "            label = self.label_list[index]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k'\n",
    "model = timm.create_model(model_name, pretrained=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import resolve_data_config\n",
    "config = resolve_data_config({}, model=model)\n",
    "CFG['mean'] =config['mean']\n",
    "CFG['std'] = config['std']\n",
    "# CFG.interpolation = config.interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    # PadSquare(value=(0, 0, 0)),\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE'] ,interpolation=cv2.INTER_CUBIC),\n",
    "    A.Normalize(mean=CFG['mean'], std=CFG['std']),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "    # PadSquare(value=(0, 0, 0)),\n",
    "    A.Resize(CFG['IMG_SIZE'], CFG['IMG_SIZE'] ,interpolation=cv2.INTER_CUBIC),\n",
    "    A.Normalize(mean=CFG['mean'], std=CFG['std']),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train['img_path'].values, train['rock_type'].values, train_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4,pin_memory=True,prefetch_factor=2)\n",
    "\n",
    "val_dataset = CustomDataset(val['img_path'].values, val['rock_type'].values, test_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4,pin_memory=True,prefetch_factor=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name , num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True).to(device)\n",
    "        self.classifier = nn.Linear(1000, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    save_path = f\"best_model_{model_name}.pth\"\n",
    "\n",
    "    for epoch in range(1, CFG['EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "\n",
    "        for imgs, labels in tqdm(iter(train_loader), desc=f\"Epoch {epoch}\"):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(imgs)\n",
    "            # print(\"output\")\n",
    "            # print(output)\n",
    "            # print(\"labels\")\n",
    "            # print(labels)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch}], Train Loss: {_train_loss:.5f}, Val Loss: {_val_loss:.5f}, Val Macro F1: {_val_score:.5f}')\n",
    "        torch.save(model.state_dict(), f\"{epoch}_{model_name}_{_val_loss:.5f}.pth\")\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "\n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "\n",
    "            # 모델 가중치 저장\n",
    "            torch.save(model.state_dict(), f\"best_{epoch}_{model_name}_{_val_loss:.5f}.pth\")\n",
    "            print(f\"Best model saved (epoch {epoch}, F1={_val_score:.4f}) → {save_path}\")\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(iter(val_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            \n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += labels.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = f1_score(true_labels, preds, average='macro')\n",
    "    \n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test.csv')\n",
    "test[\"img_path\"] = test[\"img_path\"].apply(lambda x : \"../data/test/\"+x.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test['img_path'].values, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eva02_large_patch14_448.mim_m38m_ft_in22k_in1k'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (backbone): Eva(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (rope): RotaryEmbeddingCat()\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x EvaBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): EvaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SwiGLU(\n",
       "          (fc1_g): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "          (fc1_x): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "          (act): SiLU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): Identity()\n",
       "    (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1000, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model load\n",
    "load_model = BaseModel(model_name , 7)\n",
    "\n",
    "# load_model = timm.create_model(model_name, pretrained=False).to(device)\n",
    "load_model.load_state_dict(torch.load(\"/mnt/sdb/sim/dacon/DDP/3_eva02_large_patch14_448.mim_m38m_ft_in22k_in1k.pth\"))\n",
    "load_model.eval()  # 평가 모드로 전환 (옵션)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModel(\n",
       "  (backbone): Eva(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "      (norm): Identity()\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (rope): RotaryEmbeddingCat()\n",
       "    (blocks): ModuleList(\n",
       "      (0-23): 24 x EvaBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): EvaAttention(\n",
       "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (norm): Identity()\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SwiGLU(\n",
       "          (fc1_g): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "          (fc1_x): Linear(in_features=1024, out_features=2730, bias=True)\n",
       "          (act): SiLU()\n",
       "          (drop1): Dropout(p=0.0, inplace=False)\n",
       "          (norm): LayerNorm((2730,), eps=1e-06, elementwise_affine=True)\n",
       "          (fc2): Linear(in_features=2730, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "    )\n",
       "    (norm): Identity()\n",
       "    (fc_norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "    (head_drop): Dropout(p=0.0, inplace=False)\n",
       "    (head): Linear(in_features=1024, out_features=1000, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1000, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.cuda(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(iter(test_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "    \n",
    "    preds = le.inverse_transform(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11876 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 98/11876 [00:24<48:27,  4.05it/s] "
     ]
    }
   ],
   "source": [
    "preds = inference(load_model, test_loader, device)\n",
    "#  2%|▏         | 61/2969 [01:00<47:51,  1.01it/s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('/mnt/sdb/sim/dacon/data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['rock_type'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./500개_05_submit_eva02_448.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
